{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Idea Generator & Refiner — **Annotated Notebook**\n",
        "\n",
        "This notebook implements a minimal **Idea Generator & Refiner** using OpenAI. \n",
        "You asked for explicit reasoning per line and per cell — so each code cell is heavily commented.\n",
        "\n",
        "### What you can do here\n",
        "- **Generate** N ideas for any domain with structured fields (title, summary, category, scores, impact_score)\n",
        "- **Refine** ideas toward a goal (feasibility / creativity / clarity)\n",
        "- **Experiment with temperature** to balance creativity vs focus\n",
        "- **(Optional) Auto-evaluate** ideas with a simple LLM judge\n",
        "\n",
        "Run cells **top-to-bottom** the first time, then re-run/modify any cell as needed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 1 — Install dependencies\n",
        "**Why this cell matters:** Ensures your environment has only the two required packages. If they're already installed, this does nothing.\n",
        "\n",
        "**Functions provided by this cell:** None — it's setup only."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "execution": {
          "allow_errors": true
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai in c:\\users\\extbaje\\documents\\prompt engineering exercise\\.venv\\lib\\site-packages (2.5.0)\n",
            "Requirement already satisfied: python-dotenv in c:\\users\\extbaje\\documents\\prompt engineering exercise\\.venv\\lib\\site-packages (1.1.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\extbaje\\documents\\prompt engineering exercise\\.venv\\lib\\site-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\extbaje\\documents\\prompt engineering exercise\\.venv\\lib\\site-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\extbaje\\documents\\prompt engineering exercise\\.venv\\lib\\site-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in c:\\users\\extbaje\\documents\\prompt engineering exercise\\.venv\\lib\\site-packages (from openai) (0.11.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\extbaje\\documents\\prompt engineering exercise\\.venv\\lib\\site-packages (from openai) (2.12.3)\n",
            "Requirement already satisfied: sniffio in c:\\users\\extbaje\\documents\\prompt engineering exercise\\.venv\\lib\\site-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in c:\\users\\extbaje\\documents\\prompt engineering exercise\\.venv\\lib\\site-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\extbaje\\documents\\prompt engineering exercise\\.venv\\lib\\site-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in c:\\users\\extbaje\\documents\\prompt engineering exercise\\.venv\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
            "Requirement already satisfied: certifi in c:\\users\\extbaje\\documents\\prompt engineering exercise\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\users\\extbaje\\documents\\prompt engineering exercise\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in c:\\users\\extbaje\\documents\\prompt engineering exercise\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\extbaje\\documents\\prompt engineering exercise\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in c:\\users\\extbaje\\documents\\prompt engineering exercise\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\extbaje\\documents\\prompt engineering exercise\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
            "Requirement already satisfied: colorama in c:\\users\\extbaje\\documents\\prompt engineering exercise\\.venv\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# %pip is Jupyter's magic to install packages into the current kernel.\n",
        "# We pin only what we need: the official OpenAI Python SDK and dotenv for .env loading.\n",
        "%pip install --upgrade openai python-dotenv\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 2 — Environment & client setup\n",
        "**Why this cell matters:** It wires up authentication and sets a default model so all subsequent cells can call the API.\n",
        "\n",
        "**Functions provided by this cell:** None — it exposes `client` and `MODEL` to other cells."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8cc773d4",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ.pop(\"OPENAI_API_KEY\", None)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25a69126",
      "metadata": {},
      "outputs": [],
      "source": [
        "print('Model:', MODEL, '| API key present:', 'OPENAI_API_KEY' in os.environ)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: gpt-4o-mini | API key present: True\n"
          ]
        }
      ],
      "source": [
        "import os, json, time, random  # os/env access; json for (de)serialization; time/random for simple retry/backoff\n",
        "from typing import List, Dict, Any  # type hints for clarity and editor help\n",
        "from dotenv import load_dotenv      # loads .env files into environment variables\n",
        "from openai import OpenAI           # official OpenAI Python SDK client\n",
        "\n",
        "load_dotenv()  # reads .env in the working directory so OPENAI_API_KEY is available\n",
        "\n",
        "# If you prefer to paste the key interactively instead of using .env, uncomment the two lines below.\n",
        "import getpass\n",
        "os.environ['OPENAI_API_KEY'] = getpass.getpass('Enter OPENAI_API_KEY (hidden): ')\n",
        "\n",
        "MODEL = os.getenv('OPENAI_MODEL', 'gpt-4o-mini')  # default model name; configurable via .env\n",
        "client = OpenAI()  # constructs a client that reads OPENAI_API_KEY from environment\n",
        "\n",
        "print('Model:', MODEL, '| API key present:', 'OPENAI_API_KEY' in os.environ)  # quick sanity check\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 3 — Taxonomy & minimal helpers\n",
        "**Why this cell matters:** These utilities keep outputs consistent, cheap, and parseable.\n",
        "\n",
        "**Functions provided:**\n",
        "- `clamp_0_10(x)`: normalize model scores into 0..10 ints\n",
        "- `impact(user_value, feasibility, novelty)`: compute a transparent 0..100 impact score\n",
        "- `chat_json(messages, temperature, max_tokens)`: single, JSON-mode OpenAI callsite\n",
        "\n",
        "We also define a fixed `CATEGORIES` list to keep labels consistent across runs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "CATEGORIES = ['Product','Feature','Process','Growth','Ops','Research','Content','Other']  # strict taxonomy to avoid drift\n",
        "\n",
        "def clamp_0_10(x) -> int:\n",
        "    \"\"\"Coerce any numeric-ish input to an int in [0,10]. Keeps scores predictable.\"\"\"\n",
        "    try:\n",
        "        return max(0, min(10, int(round(float(x)))))  # round then clamp to range\n",
        "    except Exception:\n",
        "        return 0  # fallback if the model returns non-numeric text\n",
        "\n",
        "def impact(user_value: int, feasibility: int, novelty: int) -> float:\n",
        "    \"\"\"Simple, explicit formula (50/30/20). You control the weights; model doesn't.\"\"\"\n",
        "    return round((0.5*user_value + 0.3*feasibility + 0.2*novelty) * 10, 1)  # scale to 0..100\n",
        "\n",
        "def chat_json(messages: List[Dict[str, str]], temperature=0.7, max_tokens=1200) -> Dict[str, Any]:\n",
        "    \"\"\"Single place to call the Chat Completions API and enforce JSON outputs.\"\"\"\n",
        "    resp = client.chat.completions.create(\n",
        "        model=MODEL,                   # which model to use\n",
        "        messages=messages,             # conversation payload (system+user)\n",
        "        temperature=temperature,       # creativity vs focus knob\n",
        "        max_tokens=max_tokens,         # hard cap to control cost\n",
        "        response_format={'type': 'json_object'},  # ask model to return a JSON object\n",
        "    )\n",
        "    return json.loads(resp.choices[0].message.content)  # parse JSON text into a Python dict\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 4 — Prompt templates (Generator, Refiner, Judge)\n",
        "**Why this cell matters:** Prompts are the contract the model must follow. We keep them explicit and JSON-oriented.\n",
        "\n",
        "**Functions provided by this cell:** None — it defines string templates referenced by later cells."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "GEN_SYS = 'You are an expert product strategist. Return ONLY valid JSON. No markdown.'  # sets role + output format\n",
        "GEN_USR = \"\"\"Domain: {domain}\n",
        "Generate {n} distinct, high-quality ideas.\n",
        "\n",
        "Each idea must include:\n",
        "- title (short, specific)\n",
        "- summary (2–4 sentences)\n",
        "- category (one of: Product, Feature, Process, Growth, Ops, Research, Content, Other)\n",
        "- user_value (0–10), feasibility (0–10), novelty (0–10)\n",
        "\n",
        "Return JSON:\n",
        "{{\n",
        "  \"ideas\": [\n",
        "    {{\n",
        "      \"title\":\"...\", \"summary\":\"...\", \"category\":\"Product\",\n",
        "      \"user_value\":7, \"feasibility\":6, \"novelty\":8\n",
        "    }}\n",
        "  ]\n",
        "}}\n",
        "\"\"\"  # tight schema example so the model knows the exact shape\n",
        "\n",
        "REF_SYS = 'You are a concise product editor. Return ONLY valid JSON. No markdown.'  # refiner role + format\n",
        "REF_USR = \"\"\"Refinement goal: {goal}  # feasibility | creativity | clarity\n",
        "Base idea:\n",
        "{idea_json}\n",
        "\n",
        "Rules:\n",
        "- Keep the core intent but improve for the stated goal.\n",
        "- Propose 3–6 concrete changes, then output ONE refined idea object.\n",
        "\n",
        "Return JSON:\n",
        "{{\n",
        "  \"changes\": [\"...\"],\n",
        "  \"refined\": {{\n",
        "    \"title\":\"...\", \"summary\":\"...\", \"category\":\"Product\",\n",
        "    \"user_value\":0, \"feasibility\":0, \"novelty\":0\n",
        "  }}\n",
        "}}\n",
        "\"\"\"  # refiner requires both a change list and a single refined object\n",
        "\n",
        "JUDGE_SYS = 'You are a strict evaluator. Return ONLY valid JSON. No markdown.'  # evaluator role\n",
        "SCORE_USR = \"\"\"Score this idea for the domain \"{domain}\" using this rubric:\n",
        "{rubric}\n",
        "\n",
        "Idea:\n",
        "{idea_json}\n",
        "\n",
        "Return JSON:\n",
        "{{ \"score\": 0.0, \"rationale\": \"1–3 sentences\" }}\n",
        "\"\"\"  # produces a numeric score and a terse rationale for auditing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 5 — Core functions (Generate, Refine, Judge)\n",
        "**Why this cell matters:** These are the actions you actually call. Each function is short and transparent.\n",
        "\n",
        "**Functions provided:**\n",
        "- `generate_ideas(domain, n, temperature)` → list of structured ideas\n",
        "- `refine_ideas(ideas, goal, temperature)` → list of refined ideas\n",
        "- `judge_score(ideas, domain, rubric, temperature)` → list of scores with rationales"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Any  # for explicit list[dict] annotations\n",
        "\n",
        "def generate_ideas(domain: str, n: int = 10, temperature: float = 0.8) -> list[dict]:\n",
        "    # Build the two-message conversation for generation\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": GEN_SYS},  # constrain the assistant\n",
        "        {\"role\": \"user\", \"content\": GEN_USR.format(domain=domain, n=n)},  # provide schema + instructions\n",
        "    ]\n",
        "    data = chat_json(messages, temperature=temperature)  # call API once\n",
        "    out: list[dict] = []\n",
        "    for it in data.get('ideas', []):  # iterate ideas returned by the model\n",
        "        uv = clamp_0_10(it.get('user_value', 0))         # normalize numeric fields\n",
        "        fe = clamp_0_10(it.get('feasibility', 0))\n",
        "        nv = clamp_0_10(it.get('novelty', 0))\n",
        "        cat = str(it.get('category', 'Other')).strip()   # sanitize category\n",
        "        if cat not in CATEGORIES:\n",
        "            cat = 'Other'\n",
        "        out.append({                                   # assemble the structured record\n",
        "            'domain': domain.strip(),\n",
        "            'title': str(it.get('title','')).strip(),\n",
        "            'summary': str(it.get('summary','')).strip(),\n",
        "            'category': cat,\n",
        "            'user_value': uv,\n",
        "            'feasibility': fe,\n",
        "            'novelty': nv,\n",
        "            'impact_score': impact(uv, fe, nv),        # compute deterministic composite\n",
        "            'temperature': float(temperature)          # log the knob used for analysis later\n",
        "        })\n",
        "    return out\n",
        "\n",
        "def refine_ideas(ideas: list[dict], goal: str = 'feasibility', temperature: float = 0.3) -> list[dict]:\n",
        "    refined = []\n",
        "    for idea in ideas:\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": REF_SYS},  # editing persona\n",
        "            {\"role\": \"user\", \"content\": REF_USR.format(goal=goal, idea_json=json.dumps(idea, ensure_ascii=False))},\n",
        "        ]\n",
        "        data = chat_json(messages, temperature=temperature, max_tokens=800)  # ask for refined object\n",
        "        r = data.get('refined', {})\n",
        "        uv = clamp_0_10(r.get('user_value', idea.get('user_value', 0)))  # keep sane defaults\n",
        "        fe = clamp_0_10(r.get('feasibility', idea.get('feasibility', 0)))\n",
        "        nv = clamp_0_10(r.get('novelty', idea.get('novelty', 0)))\n",
        "        cat = str(r.get('category', idea.get('category', 'Other'))).strip()\n",
        "        if cat not in CATEGORIES:\n",
        "            cat = 'Other'\n",
        "        refined.append({\n",
        "            'domain': idea.get('domain', ''),\n",
        "            'title': str(r.get('title', idea.get('title',''))).strip(),\n",
        "            'summary': str(r.get('summary', idea.get('summary',''))).strip(),\n",
        "            'category': cat,\n",
        "            'user_value': uv,\n",
        "            'feasibility': fe,\n",
        "            'novelty': nv,\n",
        "            'impact_score': impact(uv, fe, nv),\n",
        "            'temperature': float(temperature)\n",
        "        })\n",
        "    return refined\n",
        "\n",
        "def judge_score(ideas: list[dict], domain: str, rubric: str, temperature: float = 0.2) -> list[dict]:\n",
        "    \"\"\"Optional auto-eval: returns judge_score + rationale for each idea.\"\"\"\n",
        "    out = []\n",
        "    for idea in ideas:\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": JUDGE_SYS},  # evaluator persona\n",
        "            {\"role\": \"user\", \"content\": SCORE_USR.format(domain=domain, rubric=rubric, idea_json=json.dumps(idea, ensure_ascii=False))},\n",
        "        ]\n",
        "        data = chat_json(messages, temperature=temperature, max_tokens=400)\n",
        "        out.append({\n",
        "            'title': idea['title'],\n",
        "            'judge_score': float(data.get('score', 0.0)),\n",
        "            'judge_rationale': str(data.get('rationale', '')).strip(),\n",
        "        })\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 6 — Generate ideas (example)\n",
        "**Why this cell matters:** This is the first end-to-end run. You can change the `domain`, `n`, and `temperature` to see different behavior.\n",
        "\n",
        "**Function provided by this cell:** None — it produces a list named `ideas` in memory for later cells."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(8,\n",
              " {'domain': 'Meal Plan',\n",
              "  'title': 'Personalized Meal Plans',\n",
              "  'summary': 'Create meal plans tailored to individual dietary needs, preferences, and health goals. Users can input their restrictions, allergies, and goals to receive a custom weekly plan, maximizing nutritional value and satisfaction.',\n",
              "  'category': 'Product',\n",
              "  'user_value': 9,\n",
              "  'feasibility': 7,\n",
              "  'novelty': 8,\n",
              "  'impact_score': 82.0,\n",
              "  'temperature': 0.8})"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "domain = 'Meal Plan'  # <-- change the domain to your target space\n",
        "ideas = generate_ideas(domain, n=8, temperature=0.8)  # higher temp -> more variety (and noise)\n",
        "len(ideas), ideas[0]  # quick peek: count and first idea\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 7 — Save to JSON/CSV\n",
        "**Why this cell matters:** Persisting artifacts lets you diff runs, share outputs, or import into Sheets/Coda.\n",
        "\n",
        "**Functions provided by this cell:** `save_json`, `save_csv`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Saved ideas.json and ideas.csv'"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import csv  # standard lib CSV for lightweight exports\n",
        "\n",
        "def save_json(path: str, rows: list[dict]):\n",
        "    with open(path, 'w', encoding='utf-8') as f:         # open the file for writing\n",
        "        json.dump(rows, f, ensure_ascii=False, indent=2)  # pretty-print JSON for readability\n",
        "\n",
        "def save_csv(path: str, rows: list[dict]):\n",
        "    if not rows:\n",
        "        return  # nothing to write\n",
        "    # Use a union of keys so the header covers all fields (avoids DictWriter key mismatch).\n",
        "    keys = sorted({k for r in rows for k in r.keys()})\n",
        "    with open(path, 'w', newline='', encoding='utf-8') as f:\n",
        "        w = csv.DictWriter(f, fieldnames=keys)  # create writer with full header\n",
        "        w.writeheader()                         # write header row\n",
        "        for r in rows:                          # write each record\n",
        "            w.writerow(r)\n",
        "\n",
        "save_json('ideas.json', ideas)\n",
        "save_csv('ideas.csv', ideas)\n",
        "\"Saved ideas.json and ideas.csv\"  # status string as cell output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 8 — Refine ideas (goal-directed)\n",
        "**Why this cell matters:** Converts fuzzy ideas into more buildable/creative/clear variants.\n",
        "\n",
        "**Function provided by this cell:** None — it creates `refined` for later analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(8,\n",
              " {'domain': 'Meal Plan',\n",
              "  'title': 'Dynamic Personalized Meal Plans',\n",
              "  'summary': 'Leverage AI to create dynamic meal plans tailored to individual dietary needs, preferences, and health goals. Users can input their restrictions, allergies, and goals to receive a custom weekly plan, enhanced with seasonal recipes, community sharing, and gamified challenges for a fun and engaging experience.',\n",
              "  'category': 'Product',\n",
              "  'user_value': 10,\n",
              "  'feasibility': 8,\n",
              "  'novelty': 9,\n",
              "  'impact_score': 92.0,\n",
              "  'temperature': 0.3})"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "goal = 'creativity'  # choose: 'feasibility' | 'creativity' | 'clarity'\n",
        "refined = refine_ideas(ideas, goal=goal, temperature=0.3)  # keep temp low for tighter edits\n",
        "\n",
        "save_json('ideas_refined.json', refined)\n",
        "save_csv('ideas_refined.csv', refined)\n",
        "len(refined), refined[0]  # confirmation + first refined item\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 9 — Temperature experiment\n",
        "**Why this cell matters:** Lets you compare creativity vs. focus by logging diversity and averages.\n",
        "\n",
        "**Function provided by this cell:** None — it outputs a small summary table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'temperature': 0.3,\n",
              "  'n': 8,\n",
              "  'unique_titles': 8,\n",
              "  'avg_impact': 73.4,\n",
              "  'avg_novelty': 7.1},\n",
              " {'temperature': 0.7,\n",
              "  'n': 8,\n",
              "  'unique_titles': 8,\n",
              "  'avg_impact': 70.8,\n",
              "  'avg_novelty': 7.2},\n",
              " {'temperature': 1.0,\n",
              "  'n': 8,\n",
              "  'unique_titles': 8,\n",
              "  'avg_impact': 73.6,\n",
              "  'avg_novelty': 7.2}]"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "temps = [0.3, 0.7, 1.0]  # pick a few values to compare\n",
        "runs: dict[float, list[dict]] = {}\n",
        "\n",
        "for t in temps:\n",
        "    runs[t] = generate_ideas(domain, n=8, temperature=t)  # generate per temperature\n",
        "\n",
        "def unique_titles(rows: list[dict]) -> int:\n",
        "    return len({r['title'].strip().lower() for r in rows})  # crude diversity measure\n",
        "\n",
        "summary_rows = []\n",
        "for t in temps:\n",
        "    rows = runs[t]\n",
        "    avg_impact = round(sum(r['impact_score'] for r in rows) / len(rows), 1)\n",
        "    avg_novelty = round(sum(r['novelty'] for r in rows) / len(rows), 1)\n",
        "    summary_rows.append({\n",
        "        'temperature': t,\n",
        "        'n': len(rows),\n",
        "        'unique_titles': unique_titles(rows),\n",
        "        'avg_impact': avg_impact,\n",
        "        'avg_novelty': avg_novelty,\n",
        "    })\n",
        "summary_rows  # inspect: higher temps usually increase diversity and novelty\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 10 — Optional auto‑evaluation (LLM‑as‑judge)\n",
        "**Why this cell matters:** A second opinion helps triage. The judge is noisy, but useful for ranking.\n",
        "\n",
        "**Function provided by this cell:** None — outputs `scores` and saves them as JSON."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'title': 'Dynamic Personalized Meal Plans',\n",
              "  'judge_score': 58.0,\n",
              "  'judge_rationale': 'The idea provides significant user value by addressing personalized dietary needs and promoting engagement through community sharing and gamification. However, the feasibility score is lowered due to potential challenges in accurately leveraging AI for dynamic meal planning. The novelty is moderate, as personalized meal plans exist but the integration of community and gamification adds a unique twist.'},\n",
              " {'title': 'Smart Grocery List & Meal Prep Assistant',\n",
              "  'judge_score': 76.0,\n",
              "  'judge_rationale': 'The idea provides significant user value by addressing meal planning and grocery shopping challenges, scoring high in user value. Feasibility is moderate due to potential technical challenges in syncing with local inventories and AI integration. Novelty is also good, but similar solutions exist, slightly lowering the score.'},\n",
              " {'title': 'Interactive Meal Prep Masterclass',\n",
              "  'judge_score': 76.0,\n",
              "  'judge_rationale': 'The idea offers high user value by providing personalized meal prep plans and community engagement, which can enhance adherence to meal plans. However, feasibility is somewhat limited due to the need for quality video production and community management. The novelty is moderate, as interactive meal prep content exists but may not be widely implemented in this format.'}]"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rubric = 'User value (50), Feasibility (30), Novelty (20). Penalize vagueness.'  # transparent scoring rule\n",
        "scores = judge_score(refined or ideas, domain=domain, rubric=rubric, temperature=0.2)  # low temp for consistency\n",
        "save_json('ideas_scored.json', scores)\n",
        "scores[:3]  # peek at first three\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 11 — Pick top‑N by judge score (optional)\n",
        "**Why this cell matters:** Quick shortlist for review.\n",
        "\n",
        "**Function provided by this cell:** None — just a sorted preview."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'title': 'Smart Grocery List & Meal Prep Assistant',\n",
              "  'judge_score': 76.0,\n",
              "  'judge_rationale': 'The idea provides significant user value by addressing meal planning and grocery shopping challenges, scoring high in user value. Feasibility is moderate due to potential technical challenges in syncing with local inventories and AI integration. Novelty is also good, but similar solutions exist, slightly lowering the score.'},\n",
              " {'title': 'Interactive Meal Prep Masterclass',\n",
              "  'judge_score': 76.0,\n",
              "  'judge_rationale': 'The idea offers high user value by providing personalized meal prep plans and community engagement, which can enhance adherence to meal plans. However, feasibility is somewhat limited due to the need for quality video production and community management. The novelty is moderate, as interactive meal prep content exists but may not be widely implemented in this format.'},\n",
              " {'title': 'Personalized Allergy-Friendly Meal Planning',\n",
              "  'judge_score': 76.0,\n",
              "  'judge_rationale': 'The idea scores high on user value as it addresses specific dietary needs, which is crucial for individuals with allergies. Feasibility is also strong due to existing technology for meal planning and community platforms. Novelty is moderate, as while personalized meal planning is common, the focus on allergy-friendliness and community engagement adds a unique twist.'},\n",
              " {'title': 'Personalized Seasonal Meal Planning',\n",
              "  'judge_score': 70.0,\n",
              "  'judge_rationale': 'The idea scores well on user value due to its focus on personalized meal plans and sustainability, which are increasingly important to consumers. However, the feasibility score is moderate as implementing a dynamic feature that integrates local produce availability may face logistical challenges. The novelty is high, but the concept could benefit from more specific details on execution.'},\n",
              " {'title': 'Interactive Community Recipe Hub',\n",
              "  'judge_score': 66.0,\n",
              "  'judge_rationale': 'The idea has high user value due to community engagement and creativity, scoring 9. However, feasibility is moderate at 6, as building a robust platform with gamification and video content can be resource-intensive. Novelty is strong at 10, but the concept lacks specific details on implementation, which affects clarity.'}]"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "topN = 5  # how many to shortlist\n",
        "sorted(scores, key=lambda x: x['judge_score'], reverse=True)[:topN]\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv (3.14.0)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
